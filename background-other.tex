
Content delivery networks (CDNs) ensure a high quality experience for the end-users, and thus play a vital role in today's Internet ecosystem.  A typical CDN  operates a set of distributed server deployments, and uses a combination of edge caching, intelligent server selection, and path and protocol optimizations for delivery of several types of content, e.g., video, bulk downloads, and interactive websites. Section 2.1 describes the content placement and request redirection schemes that are commonly used by CDNs today. Several CDNs today use P2P content distribution to supplement the server infrastructure of the CDN.  P2P content distribution provides significant cost savings to CDNs and is a highly scalable. Section 2.2 gives a brief overview of the work on P2P content distribution, focusing on the BitTorrent protocol.
%Several CDNs have implemented client-assisted content delivery technology 
%Section 2.1 gives an overview of the work on peer-to-peer content distribution, especially the BitTorrent protocol. 
%After the enormous popularity of peer-to-peer content distribution, BitTorrent protocol 

Complementary to CDNs, network operators optimize routing in their network to minimize congestion and balance load on their links. These route optimizations are referred to as traffic engineering; Section 2.3 provides an overview of several classes of traffic engineering schemes. 
A recent line of research has explored the interaction of traffic engineering and content distribution, which is described briefly in Section 2.4. The research seeks to find approaches for cooperative traffic engineering and content distribution, to benefit both networks and CDNs. The necessity of CDNs in the Internet is largely because the Internet itself was never designed to provide high-quality content delivery. Few recent efforts, in particular CCN, have proposed a complete redesign of the Internet protocols to reflect content delivery goals. A brief overview of these architectures is provided in Section 2.5.



%The Internet was designed to provide a high-quality content delivery. 









\section{Content placement}



%{\subsubsection{Content Distribution}} 
A traditional CDN has two key decision components---{\em content placement} and {\em request redirection}---that seek to optimize the response time perceived by end-users and balance the load across its content servers. Content placement decides which objects should be cached at which nodes. An object may be stored at multiple nodes in the network or not stored in the network at all and be served from the origin server instead. Request redirection determines which server storing a replica of the object is best positioned to serve it.

Content placement schemes can either be {\em \planned} or {\em \unplanned}. A \planned\ scheme calculates placement using a {\em content matrix} that specifies the demand for each content at each location. The content matrix is learned by monitoring a recent history of system-wide requests and possibly including hints, if any, from content providers about anticipated demand for some objects. A \planned\ scheme uses a recent content matrix to decide on a placement periodically (e.g., daily) but does not alter its placement in between. In contrast, an \unplanned\ scheme can continually alter its placement potentially even after every single request. A simple and widely used example of an \unplanned\ placement scheme is LRU, where each server evicts the least-recently-used object from its cache to make room for new ones.


\section{Request redirection}

Request redirection (or the server selection) is dependent on three factors:  round trip time and the loss rate between the client and the server, current load on the server, and CDN policies \cite{donar,DilleyMPPSW02,oasis}.  A general approach to solve this problem described in \cite{donar} is to estimate a cost function value for  every server based on  round trip time, loss rates, and the current server load.  
The cost function values for all servers and  the CDN policy, e.g. redirect to closest server, or load balance among three closest servers,  determine which server receives each request.


\section{Server bandwidth}

Summary of swarming systems, features of which have been adopted by many CDNs today.

\section{Route optimization} 
A key component of ISP network operations is traffic engineering, which seeks to route the traffic demands through the backbone network so as to balance the load and mitigate hotspots. Traffic engineering is commonly viewed as a routing problem that takes as input a {\em traffic matrix}, i.e., the aggregate flow demand between every pair of PoPs observed over a recent history, and computes routes so as to minimize a network-wide cost objective. The cost seeks to capture the severity of load imbalance in the network and common objective functions include the maximum link utilization (MLU) or a convex function (so as to penalize higher utilization more) of the link utilization aggregated across all links in the network \cite{fortz2000internet}. ISPs commonly achieve the computed routing either by using shortest-path routing (e.g., the widely deployed OSPF protocol \cite{fortz2000internet}) or by explicitly establishing virtual circuits (e.g., using MPLS \cite{MPLS2}). ISPs perform traffic engineering at most a few times each day, e.g., morning and evening each day \cite{InvCap}.

Routing can also be classified as {\em \planned} or {\em \unplanned} similar in spirit to content placement. Traffic engineering schemes as explained above are implicitly \planned\ as they optimize routing for recently observed demand. To keep the terminology simple, we also classify online traffic engineering schemes \cite{TEXCP,MPLS2} (that are rarely deployed today) as \planned. In contrast, \unplanned\ routing schemes are simpler and rely upon statically configured routes \cite{Cohen,Racke}, e.g., InverseCap is a static shortest-path routing scheme that sets link weights to the inverse of their capacities; this is a common default weight setting for OSPF in commercial routers \cite{InvCap}.


Recent work has explored the joint optimization of traffic engineering and ``content distribution'', where the latter term refers to the {\em server selection} problem.
P4P proposed by Xie et al.  \cite{P4P} seeks to improve application performance for peer-to-peer (P2P) traffic while also reducing cost for the ISP. P4P assumes a cooperative model where the ISP supplies hints called {\em p-distances} to P2P applications that when used by them improves their performance and also reduces interdomain transit costs and the MLU for the ISP. In  \cite{Jiang2009} and  \cite{JohariGameTheory}, the authors study the interaction between traffic engineering and content distribution using a game-theoretic model and show that, without a joint optimization, the equilibrium of this interaction may not result in a socially optimal solution. In  \cite{Jiang2009}, it is shown that a joint optimization can achieve benefits of up to 20\% for ISPs and up to 30\% for CDNs as compared to the case when there is no cooperation between them. 


%\section{Mapping}
%
%Mapping clients to server locations. 

%(3) Research proposals from the networking community (i) a better co-operation of content delivery and network management tasks today. (ii) Clean-slate redesign of networking to imbibe content delivery goals. 


\section{Dynamic Data Placement}

How can the data backend be partitioned.
Applications whose data-backend has locality can benefit from this.
Limited benefit in some cases. e.g. amazon inventory list. 
Has already been demonstrated by Volley => lower latency, lower bandwidth cost, lower storage.

If all data is not available everywhere how do you find out the location of a data item.


%\section{VM Placement}
%
%\section{Data center traffic engineering}






%1. interaction of content delivery and traffic engineering 
%
%(1) static content --- stresses network bandwidth, storage constraints
%
%(2) dynamic content --- stresses server resources
%
%Challenges:
%
%scalability
%
%a. content delivery concerns should be kept separate from network management concerns, overlay approach is fine.
%
%b. leverage client resources to achieve greater scalability, while contributing minimal server bandwidth.
%
%c. service placement 
%
%d. 
