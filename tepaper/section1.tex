\chapter{An application-centric comparison of ISP traffic engineering schemes}
\label{ch:beyondmlu}

Traffic engineering (TE) techniques are used by Internet Service Providers (ISPs) for computing network routing based on expected traffic demands. Our work focus on two aspects of ISP traffic engineering that have received less attention in prior work and are of key importance in a content-dominated network. First, the impact of traffic engineering on user-perceived peformance such as TCP file download times. Second, the impact of application adaptation on traffic engineering. In particular we focus on a form of adaptation enabled by \emph{location diversity}, or the ability of applications to download content from multiple locations. Location diversity is prevalent in the Internet due to several applications and services such as content delivery networks, peer-to-peer applications and mirrored websites.

While ISP traffic engineering schemes are commonly evaluated using link-utilization based metrics, these metrics are ill-suited to compare schemes based on user-perceived performance or their network capacity, i.e., the factor of surges in traffic demand that a network can tolerate, under location diversity. As discussed previously in Chapter \ref{ch:te-background}, link utilization-based metrics may not reflect application performance that depends on other metrics such as propagtion delay, access link capacity and backbone link capacity also. Further, link utilization based metrics, in particular MLU, do not reflect network capacity under location diversity as explained before. 

There are two key parts of our approach to evaluate a TE scheme. First, we carefully and at scale simulate network traffic as a collection of TCP flows. This empirical approach  enables us to accurately measure application performance metrics --- TCP throughput for elastic traffic and a quality-of-service metric,  MOS score for VoIP quality \cite{MOS-formula}, for inelastic traffic --- and model application adaptation to location diversity, e.g., how an application splits traffic among multiple content locations. Second, we use a novel capacity metric called Surge Protection Factor (SPF). Unlike MLU, SPF captures the factor of increase in demand that can be sustained while accounting for location diversity. 

We show following results from comparing user-perceived performance of TE schemes based on real network topologies and traffic demands. All state-of-the-art TE schemes achieve nearly identical application performance at typical Internet load levels. In fact, even a demand-oblivious TE scheme --- static shortest-path routing with link weights inversely proportional to the capacity (\invcap) (i.e., no engineering at all) --- achieves the same application performance as optimal TE. Ironically, demand-aware TE schemes that engineer for unexpected traffic spikes (e.g., COPE \cite{COPE}) consistently hurt TCP throughput despite achieving near-optimal MLU. Overall, these results show that link utilization based metrics are poorly correlated with user-perceived performance.

We show following results from comparing SPF achieved by TE schemes. The optimal TE scheme with perfect knowledge of traffic matrix and sub-optimal TE schemes like demand-aware TE by optimizing OSPF link weights \cite{fortz2000internet} end-up achieving the same SPF. Even the demand-oblivious scheme, \invcap, achieves an SPF at most 30\% worse than optimal TE. Overall, these results show that application adaptation to location diversity blurs differences between traffic engineering schemes.

The rest of the chapter is as follows. Section \ref{sec:locdiv-background} introduces location diversity and proposes a new capacity metric SPF to compare traffic engineering schemes. Section \ref{sec:exp_setup} presents our simulation setup. Section \ref{sec:app_performance} compares the application performance of TE schemes and Section \ref{sec:capacity} compares their achieved capacity under location diversity. 


%Our approach to evaluating traffic engineering is empirical and application-centric.  Our experiments carefully and at scale simulate end-to-end application behavior. This approach enables us to evaluate TE schemes with respect to their impact on application performance in terms of TCP throughput for elastic traffic and a quality-of-service metric,  MOS score for VoIP quality \cite{MOS-formula}, for inelastic traffic. Further, this approach enables us to model application adaptation to location diversity and measure capacity using a novel capacity metric called Surge Protection Factor (SPF). SPF captures the factor of increase in demand that can be sustained while accounting for location diversity.

 
%Further, this approach enables us to simulate application adaptation to location diversity. 


%Our research shows that link utilization based metrics, in particular the maximum link utilization (MLU) is a poor metric to evaluate user-perceived performance or \emph{network capacity}, i.e., the maximum surge in traffic demand that a network can support. Our results support our previous hypothesis that link utilization based metrics may not reflect user-perceived performance that depends on other factors also.
%Previously, we explained why link utilization based metrics may not reflect user-perceived performance that depends on other factors also.  Previously, we explained why MLU is no longer an effective metric of capacity in the presence of location diversity. In this Chapter, we propose a new capacity metric called Surge Protection Factor (SPF) for comparing traffic engineering schemes. SPF captures the factor of increase in demand that can be sustained while accounting for location diversity.


\eat{
	Traditionally, traffic engineering (TE) has been studied as an optimization problem that takes as input a traffic matrix (TM) and seeks to compute routes so as to minimize a network cost function. The cost function is intended to capture the severity of congestion hotpsots based on link utilization levels. For example, the most widely used cost function, MLU, is simply the utilization of the most utilized link in the network \cite{COPE,TEXCP,MultiTM,Cohen}; others  sum over all links a convex function of their utilization  (so as to penalize highly utilized links more)  \cite{fortz2000internet,fortz2002traffic}. There are two implicit assumptions underlying this line of work. First, maintaining low link utilization improves user-perceived performance under typical load conditions. Second, maintaining low link utilization increases the effective capacity of the network by enabling it to accommodate unexpected surges in the traffic demand.
	
	
	
	Our work questions both of the above assumptions. The distinguishing aspect of our work is an application-centric approach to the problem: instead of posing TE as as optimization problem seeking to minimize link utilization, we focus on application performance metrics such as TCP throughput for elastic  traffic and quality-of-service metrics (e.g., MOS score for VoIP quality \cite{MOS-formula}) for inelastic traffic. Accordingly, our evaluation methodology is empirical: instead of relying on mathematical simulations based on linear programming or heuristic techniques for NP-complete problems, our experiments carefully and at scale simulate end-to-end application behavior so as to compare TE schemes with respect to their impact on application performance. 
	
	Our application-centric and empirical approach reveals rather unexpected results. Our first finding is that metrics based on link utilization alone, and in particular MLU, are a poor proxy for application performance. For example, a TE scheme may incur twice the MLU of another TE scheme and yet achieve as good or better application performance. The key reason for this mismatch is that application performance is largely determined by end-to-end loss rate and delay, but link utilization does not capture them accurately. At typical Internet loads, and in fact until the utilization starts approaching the capacity, link loss rates remain negligibly small. This observation has also been confirmed by explicit measurements on Internet backbones \cite{ExpRouterBuffer}, and is consistent with studies on ISP backbones showing that over 90\% of all packet loss is caused by interdomain routing fluctuations as opposed to high utilization \cite{SprintStudy} and 90\% of TCP flows experience no packet loss \cite{SprintBackbone}. Furthermore, end-to-end Internet path delays are known to be largely determined by propagation delays as opposed to queueing delays \cite{SprintBackbone,SingleHopDelay}. 
	
	As a result, we find that all state-of-the-art TE schemes achieve nearly identical application performance at typical Internet load levels. In fact, even static shortest-path routing with link weights inversely proportional to the capacity (\invcap) (i.e., no engineering at all) achieves the same application performance as optimal TE. Ironically, TE schemes that engineer for unexpected traffic spikes (e.g., COPE \cite{COPE}) consistently hurt TCP throughput despite achieving near-optimal MLU.
	
	More surprisingly, we find that application adaptation to location diversity,  i.e., the ability to download content from multiple potential locations, blurs differences even in the achieved capacities of different TE schemes enabling all of them to be near-optimal. With location diversity, we find that the inverse of the MLU is no longer a meaningful metric of capacity. Instead, we formalize a new metric of the capacity achieved by a TE scheme called the {\em surge protection factor} (SPF) that captures the factor of increase in demand that can be sustained while accounting for location diversity. TE schemes calculate routing based on a measured traffic matrix to achieve a desired network cost, but application adaptation to location diversity changes the traffic matrix itself in response to a change in routing resulting a different network cost than expected. As a result, the optimal TE scheme with perfect knowledge of traffic matrix and sub-optimal TE schemes like OSPF weight-tuning \cite{fortz2000internet} end-up achieving the same SPF. Even the static routing scheme, \invcap, achieves an SPF at most 30\% worse than optimal TE.
}
