\chapter{An application-centric comparison of ISP traffic engineering schemes}
\label{ch:beyondmlu}

Traffic engineering (TE) techniques are used by Internet Service Providers (ISPs) for computing network routing based on expected traffic demands \cite{rexford}. Our work focus on two aspects of ISP traffic engineering that have received less attention in prior work and are of key importance in a content-dominated network. The first is the impact of traffic engineering on user-perceived peformance such as TCP file download times. The second is the impact of application adaptation on traffic engineering. In particular we focus on a form of adaptation enabled by \emph{location diversity}, or the ability of applications to download content from multiple locations. Location diversity is prevalent in the Internet due to several applications and services such as content delivery networks, peer-to-peer applications and mirrored websites \cite{akamai-overview,bittorrent}.

While ISP traffic engineering schemes are commonly evaluated using link-utilization based metrics, these metrics are ill-suited to compare schemes based on user-perceived performance or network capacity, i.e., the factor of surge in traffic demand that a network can tolerate, under location diversity. As discussed previously in Chapter \ref{ch:te-background}, link utilization-based metrics may not reflect application performance that depends on other metrics such as propagtion delay, access link capacity and backbone link capacity also. Further, link utilization based-metrics, in particular MLU, do not reflect network capacity under location diversity. 

There are two key parts of our approach to evaluate a TE scheme. First, we carefully and at scale simulate network traffic as a collection of TCP flows. This empirical approach  enables us to accurately measure application performance metrics --- TCP throughput for elastic traffic and a quality-of-service metric,  MOS score for VoIP quality \cite{MOS-formula}, for inelastic traffic --- and model application adaptation to location diversity, e.g., how an application splits traffic among multiple content locations. Second, we propose a new capacity metric called Surge Protection Factor (SPF). Unlike MLU, SPF captures the factor of increase in demand that can be sustained while accounting for location diversity.

We show following results from comparing user-perceived performance of TE schemes based on real network topologies and traffic demands. All state-of-the-art TE schemes achieve nearly identical application performance at typical Internet load levels. In fact, even a demand-oblivious TE scheme --- static shortest-path routing with link weights inversely proportional to the capacity (\invcap) (i.e., no engineering at all) --- achieves the same application performance as optimal TE. Ironically, demand-aware TE schemes that engineer for unexpected traffic spikes (e.g., COPE \cite{COPE}) consistently hurt TCP throughput despite achieving near-optimal MLU. Overall, these results show that link utilization based metrics are poorly correlated with user-perceived performance.

The explanation for the above results is that link loss rates and queuing delays remain negligibly small at typical Internet loads, and in fact until the utilization starts approaching the capacity. This observation above also been confirmed by explicit measurements on Internet backbones \cite{ExpRouterBuffer}, and is consistent with studies on ISP backbones showing that over 90\% of all packet loss is caused by interdomain routing fluctuations as opposed to high utilization \cite{SprintStudy} and 90\% of TCP flows experience no packet loss \cite{SprintBackbone}. Furthermore, end-to-end Internet path delays are known to be largely determined by propagation delays as opposed to queueing delays \cite{SprintBackbone,SingleHopDelay}.


Our comparison of the SPF of TE schemes in which application leverage location diversity by downloading content in parallel from all locations shows following somewhat surprising results. A small degree of location diversity (2-4 locations) 
increases SPF achieved by TE schemes by up to 2$\times$. But, the capacity increase benefits sub-optimal TE schemes, e.g., demand-aware TE via OSPF link weight optimization  \cite{fortz2000internet}, much more than the optimal TE scheme. As a result, all demand-aware TE schemes end-up achieving the same SPF as the optimal TE scheme. Even the demand-oblivious scheme, \invcap, achieves an SPF at most 30\% worse than optimal TE. 

We also experiment with a second adaptation scheme in which users download content from a single location with the smallest propagation delay. 
In these experiments, an increase in location diversity yields little improvement in SPF of any TE scheme. This adaptation reduces the SPF of \opt\ to bridge the gap between \opt\ and sub-optimal schemes such as \optwt.
Overall, these results show that application adaptation to location diversity signficantly reduces the differences between TE schemes.


The rest of the chapter is organized as follows. Section \ref{sec:locdiv-background} introduces location diversity and proposes a new capacity metric SPF to compare traffic engineering schemes. Section \ref{sec:exp_setup} presents our simulation setup. Section \ref{sec:app_performance} compares the application performance of TE schemes and Section \ref{sec:capacity} compares their achieved capacity under location diversity.


\eat{
	Traditionally, traffic engineering (TE) has been studied as an optimization problem that takes as input a traffic matrix (TM) and seeks to compute routes so as to minimize a network cost function. The cost function is intended to capture the severity of congestion hotpsots based on link utilization levels. For example, the most widely used cost function, MLU, is simply the utilization of the most utilized link in the network \cite{COPE,TEXCP,MultiTM,Cohen}; others  sum over all links a convex function of their utilization  (so as to penalize highly utilized links more)  \cite{fortz2000internet,fortz2002traffic}. There are two implicit assumptions underlying this line of work. First, maintaining low link utilization improves user-perceived performance under typical load conditions. Second, maintaining low link utilization increases the effective capacity of the network by enabling it to accommodate unexpected surges in the traffic demand.
	
	Our work questions both of the above assumptions. The distinguishing aspect of our work is an application-centric approach to the problem: instead of posing TE as as optimization problem seeking to minimize link utilization, we focus on application performance metrics such as TCP throughput for elastic  traffic and quality-of-service metrics (e.g., MOS score for VoIP quality \cite{MOS-formula}) for inelastic traffic. Accordingly, our evaluation methodology is empirical: instead of relying on mathematical simulations based on linear programming or heuristic techniques for NP-complete problems, our experiments carefully and at scale simulate end-to-end application behavior so as to compare TE schemes with respect to their impact on application performance. 
	
	Our application-centric and empirical approach reveals rather unexpected results. Our first finding is that metrics based on link utilization alone, and in particular MLU, are a poor proxy for application performance. For example, a TE scheme may incur twice the MLU of another TE scheme and yet achieve as good or better application performance. The key reason for this mismatch is that application performance is largely determined by end-to-end loss rate and delay, but link utilization does not capture them accurately. At typical Internet loads, and in fact until the utilization starts approaching the capacity, link loss rates remain negligibly small. This observation has also been confirmed by explicit measurements on Internet backbones \cite{ExpRouterBuffer}, and is consistent with studies on ISP backbones showing that over 90\% of all packet loss is caused by interdomain routing fluctuations as opposed to high utilization \cite{SprintStudy} and 90\% of TCP flows experience no packet loss \cite{SprintBackbone}. Furthermore, end-to-end Internet path delays are known to be largely determined by propagation delays as opposed to queueing delays \cite{SprintBackbone,SingleHopDelay}. 
	
	As a result, we find that all state-of-the-art TE schemes achieve nearly identical application performance at typical Internet load levels. In fact, even static shortest-path routing with link weights inversely proportional to the capacity (\invcap) (i.e., no engineering at all) achieves the same application performance as optimal TE. Ironically, TE schemes that engineer for unexpected traffic spikes (e.g., COPE \cite{COPE}) consistently hurt TCP throughput despite achieving near-optimal MLU.
	
	More surprisingly, we find that application adaptation to location diversity,  i.e., the ability to download content from multiple potential locations, blurs differences even in the achieved capacities of different TE schemes enabling all of them to be near-optimal. With location diversity, we find that the inverse of the MLU is no longer a meaningful metric of capacity. Instead, we formalize a new metric of the capacity achieved by a TE scheme called the {\em surge protection factor} (SPF) that captures the factor of increase in demand that can be sustained while accounting for location diversity. TE schemes calculate routing based on a measured traffic matrix to achieve a desired network cost, but application adaptation to location diversity changes the traffic matrix itself in response to a change in routing resulting a different network cost than expected. As a result, the optimal TE scheme with perfect knowledge of traffic matrix and sub-optimal TE schemes like OSPF weight-tuning \cite{fortz2000internet} end-up achieving the same SPF. Even the static routing scheme, \invcap, achieves an SPF at most 30\% worse than optimal TE.
}
