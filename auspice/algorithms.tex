

\subsection{\auspice\ design and implementation}
\label{sec:design}

In this section, we present the design of \auspice, its underlying optimization framework, and implementation and instantiation of a global name service using \auspice.

Figure \ref{TBD} illustrates the architecture of \auspice. The {\em mapper} is responsible for determining where to place service replicas. The mapper proceeds in epochs and recomputes the replica locations for all services at the end of each epoch. In each epoch, the mapper obtains summarized load reports for each service from all of the current replicas of the service. A summarized load report from a replica contains the request rate for that service seen by that replica from each geographic region. To this end, end-users across all of the hosted services are partitioned into non-overlapping geographic regions referred to as {\em user-groups}. Each replica's summary report consists of a spatial vector of request rates where the $i$'th component is the request rate from user-group $i$.

Each service maintains a fixed number, $K$ of {\em primary} replicas and a variable number of {\em active} replicas. The locations of the primaries are fixed and are computed using $K$ random but deterministic hash functions each of which map the name of the service on to the set of all available hosting locations. For each service, the primaries form the ``control plane'' and are responsible for making replication and placement decisions, while the active replicas are responsible for actually processing requests from end-users.  The set of primary and active replica locations for a service need not be overlapping, i.e., a single physical replica could play the logically separate roles of a primary and active replica for that service.

In each epoch, the primary replicas of a service aggregate all of summarized load reports corresponding to each of the active replicas. The primaries submit these as input to the mapper that returns a set of active replica locations for that service for the next epoch. The primaries execute the Paxos consensus algorithm to ensure that all primaries agree on the placement for the next epoch computed by the mapper. During periods of graceful execution, only one primary (the coordinator in Paxos) actually invokes the mapper while the other primaries simply accept this value. However, consensus is necessary to ensure that all primaries have a consistent view of the current active replica locations for the service. 

The mapper can be invoked either in an {\em uncoordinated} mode or {\em coordinated} mode. The uncoordinated mapper is implemented as a library and is invoked locally by the primaries, so the placement decision of each service is made independent of other services. The coordinated mapper is implemented as a logically centralized service maintained by the hosting service provider, which enables the mapper to optimize placement decisions across all services.


\begin{table}[ht]
\center
\begin{small}
\begin{tabular}{p{2.90in}}
\hline
\textbf{Parameters} \\

\end{tabular}
\begin{tabular}{p{0.25in}|p{2.5in}}
\hline
$U$ &  Set of geographic regions spanning all users \\ 
$D$ & Set of distinct hosting locations\\
$S$ & Set of all hosted services\\
$C_d$ & Capacity (maximum query rate) of server $d \in D$\\
$L_{ud} $ & Average latency between users in region $u \in U$ and server location $d \in D$\\
$R_{us}$ & Lookup query ate of service $s \in S$ from users in region $u \in U$\\ 
$W_s$ & Update query rate of service $s \in S$\\
$B$ & Minimum number of replicas of each service\\
\hline
\end{tabular}

\vspace{0.1in}
\begin{tabular}{p{2.9in}}
\hline
\textbf{Variables} \\
\end{tabular}
\begin{tabular}{p{0.25in}|p{2.5in}}
\hline
$x_{ds}$ & Binary variable indicating whether service $s\in S$ is replicated at $d \in D$.\\
$y_{uds}$ & At location $d \in D$, query rate of users $u \in U$ for service $s \in S$\\
\hline
\end{tabular}
\end{small}
\caption{\auspice\ parameters and variables}
\label{table:parameters}
\end{table}


\subsection{Placement optimization algorithms}

In this section, we first present a {\em coordinated} placement algorithm based on an optimization approach and then present {\em uncoordinated} heuristic algorithms that moderately trade off latency or resource usage cost in exchange for a simpler and efficient implementation.

\subsubsection{Model}

All variables used in this formulation are described in Table \ref{table:parameters}. Let $D$ be the set of locations of server deployments for a cloud service provider, and $C_d$ the total capacity of deployed servers at location $d \in D$.

%Users requesting these services are located at distinct geographic regions. We partition the  geographic area spanning all the users, e.g. a continent or the entire world,  into non-overlapping geographic regions $U$.  Instead of calculating the latency to each user individually, we consider the average latency between users in a geographic region (called a \emph{user-group}) and each server location: $L_{ud}$ is the average latency between user-group  $u \in U$ and the deployment $d \in D$.

Users requesting these services are partitioned into distinct, geographically distributed network regions or user-groups $U$. The user-groups are assumed to be fine-grained enough so that the latency from any member of a user-group $u\in U$ to a service location $d$ is close to the average latency $L_{ud}$ from users in $u$ to $d$.

%We partition the  geographic area spanning all the users, e.g. a continent or the entire world,  into non-overlapping geographic regions $U$.  Instead of calculating the latency to each user individually, we consider the average latency between users in a geographic region (called a \emph{user-group}) and each server location: $L_{ud}$ is the average latency between user-group  $u \in U$ and the deployment $d \in D$.


Let $S$ denote the set of all services. Each service $s \in S$ receives an aggregate query rate of  $r_{us}$ from user-group $u \in U$. Replicas of a service coordinate with each other upon an update to ensure consistency. Let $w_s$ denote the update rate experienced by service $s$. Each service is replicated at at least $B$ locations to meet the service's fault-tolerance and availability objectives.

The system decides the placement of service replicas and also decides to which replica to redirect each user.  A user's request is assumed to be serviceable from any of the service's replicas.  If a replica of service $s\in S$ is placed at location $d \in D$, the corresponding (binary) decision variable $x_{sd}$ takes the value 1,  otherwise $x_{sd}$ equals zero. The volume of queries from user-group $u \in U$ to the replica (if any) at location $d \in D$ of a service $s \in S$ is denoted by  $y_{uds}$,  a decision variable that takes values between $0$ and $r_{us}$.


\eat{
Our goal is to minimize the average latency between users and service replicas across all users in the system.  We ensure availability for all services by replicating each service at $B$ locations or more. 
}

\subsection{Optimization formulation}

Minimizing the average latency can be formulated as a mixed integer program. The following objective minimizes the aggregate latency across all users' requests.


\begin{eqnarray}
\text{minimize:}\ \sum_{s \in S} \sum_{d \in D} \sum_{u \in U} L_{ud} \ y_{uds}
\end{eqnarray}

The optimization must satisfy the constraints of the problem specified from Equation (2) to Equation (7).

All users' requests must be satisfied.
\begin{eqnarray}
\sum_{d \in D} {y_{uds}} = R_{us} \quad \forall u \in U, s \in S
\end{eqnarray}

The server capacity at each location must be greater than the total  query rate of users' and the update rate of  service replicas  placed at that  location.
\begin{eqnarray}
\sum_{s \in S} \sum_{u \in U} y_{uds} + \sum_{s \in S} W_s\ x_{ds} \leq C_d \quad \forall d \in D
\end{eqnarray}

To ensure availability, each service should be replicated at $B$ locations or more. 
\begin{eqnarray}
\sum_{d \in D} x_{ds} \geq B \quad \forall s \in S
\end{eqnarray}

A request can be served from a location only if a replica of the service is available at that location.
\begin{eqnarray}
y_{uds} \leq x_{ds} R_{us}\quad \forall u \in U, d \in D , s \in S
\end{eqnarray}


The next two equations constrain the values each variable can take.
\begin{eqnarray}
x_{ds} \in \{0, 1\} \quad \forall d \in D, s \in S
\end{eqnarray}
\begin{eqnarray}
0 \leq y_{uds} \leq R_{us} \quad \forall u \in U, d \in D , s \in S
\end{eqnarray}

\eat{
TODO:

(1) remove the capacity constraint, formulate it as a LP problem and solve it using CPLEX tool

(2) Either prove it is a NP hard, or show it is not

(3) Constraints: bandwidth and computing/processing capacity at each server
}

\subsubsection{Server request processing delay}

The above formulation does not optimize for request processing delay at the server, or \emph{server latency} for short. We describe an extension to the above formulation that optimizes for the overall user latency including network latency and server latency given a load-vs-response-time behavior for each service at each server is known.

Let $t_d$ be the total request rate at location $d \in D$. From equation (3),
\begin{eqnarray}
t_d = \sum_{s \in S} \sum_{u \in U} y_{uds} + \sum_{s \in S} W_s\ x_{ds} 
\end{eqnarray}
The server utilization at $d \in D$ is $t_d / C_d$.  We define the server latency per request as a function  of server utilization. The function $f$ is shown below.
\begin{eqnarray}
f (t_d / C_d)   =  \begin{cases}  r_1 & \text{if } 0 \leq t_d / C_d \leq u_1,  \notag \\
r_2 & \text{if } u_1 < t_d / C_d \leq u_2,  \notag \\
... & \notag \\
r_j & \text{if } u_{j - 1} < t_d / C_d \leq 1  \notag \end{cases}\\
\end{eqnarray}
Server latency increases with server utilization, so that $0 \leq r_1  < ... < r_j$. Essentially, the above transforms a load vs. response time curve to a piecewise-linear, convex function, a technique that has also been in used in other domains such as traffic engineering via optimization of OSPF weights \cite{fortz2000Internet} to make the optimization linear.

Let $M_d$ be the total server latency at location $d \in D$. $M_d$ is defined  by the following set of equations.
\begin{eqnarray}
v_0 = 0, u_0 = 0, u_j = 1\\
M_d \geq v_{i - 1} + r_i\ (t_d - u_{i -1} \ C_d) \quad \forall i \in \{1, 2, ..., j\}  \\
v_i = v_{i - 1} + r_i\ (u_i - u_{i - 1})\ C_d \quad \forall i \in \{1, 2, ..., j\} 
\end{eqnarray}

Finally, we incorporate the total server latency $M_d$ at each location $d\in D$ into the objective function.


\begin{eqnarray}
\text{minimize:}\  \sum_{s \in S} \sum_{d \in D} \sum_{u \in U} L_{ud} \ x_{ds} \ y_{uds}  + \sum_{d \in D} M_d
\end{eqnarray}


\subsubsection{Computational hardness}

Mixed integer programs are generally computationally expensive to solve. We show that the service placement problem is NP-Complete via a reduction from the partition problem, with the proof deferred to Appendix \ref{appendix}. Thus, computing the optimal solution intrinsically demands a computationally expensive approach, motivating simpler heuristic placement algorithms.

%As the problem is  NP-Hard,  this problem can be solved optimally only via  computationally expensive approach such as the MIP presented above.


\subsubsection{Heuristics for service placement}

%Solving the MIP for practical use cases may take prohibitively long for it to be effective, e.g.,  solving the MIP for 1000 services and 100 server locations takes Y hours to finish on a server class machine with an industrial strength MIP solver. We evaluate heuristics for this problem, which are sub-optimal but are an order of magnitude faster than the MIP. Three heuristics that we consider are: \uniform, \kmedoids, \locaware.

\auspice\ supports three heuristic algorithms respectively referred to as \uniform, \kmedoids, \locaware. In each heuristic, the number of  replicas of a service equals  $M \times \text{total-query-rate}/\text{update-rate}$, where M is a configurable parameter. Each heuristic chooses a different policy to select locations of these replicas. 

\uniform\ is the simplest and randomly selects the given number of replica locations. \locaware\ selects up to N  replica locations using a voting scheme for locality-awareness and more replicas, if any, are chosen randomly. The voting scheme assigns one vote to each user that she uses to vote for the server location closest to her.  \locaware\ chooses the top $N$ locations that received the most number of votes during a recent epoch. \kmedoids\ is a greedy, iterative clustering algorithm \cite{kmedoids} that, similar in spirit to the k-means algorithm, begins by designating $k$ randomly chosen locations as ``medoids'', assigns the remaining points to their closest medoid, and in each iteration greedily swaps the medoid in each cluster with the point that results in the lowest cost, i.e., the sum of distances of all points from the medoid.


The \beehive\ replication framework determines the placement of name record replicas in the Codons name service. 
The number of replicas of an object is calculated based on its popularity ranking, with more popular objects replicated at greater number of  locations. 
The location of objects are decided by consistent hashing.
Our \beehive\ implementation retains these features.
However, we do not implement a DHT-based overlay routing for \beehive.
Each request is routed the closet service replica in terms of latency,  instead of routing to the replica with the closet ID as in a DHT, so our latency numbers are a conservative estimate of the latency in Codons.


%How these are implemented as a distributed algorithm?

