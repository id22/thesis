%!TEX root = New.tex
\chapter{Related Work}


\section{Content-centric networking}

\section{Network CDNs}
\label{sec:ncdn-related}

Traffic engineering and content distribution have both seen an enormous body of work over more than a decade. To our knowledge, our work is the first to pose the \ncp\ problem, wherein a single entity seeks to address both concerns, and empirically evaluate different content-aware traffic engineering strategies. Nevertheless, a significant body of recent research has studied the interaction of content distribution and traffic engineering in various forms, and we explain below how our work relates to and builds upon this prior work.

%\noindent\textbf{Interaction of location diversity and TE:}

{\paragraph{Joint Optimization} Recent work has explored the joint optimization of traffic engineering and "content distribution'', where the latter term refers to the {\em server selection} problem.
P4P proposed by Xie et al.  \cite{P4P} seeks to improve application performance for peer-to-peer (P2P) traffic while also reducing cost for the ISP. P4P assumes a cooperative model where the ISP supplies hints called {\em p-distances} to P2P applications that when used by them improves their performance and also reduces interdomain transit costs and the MLU for the ISP. In  \cite{Jiang2009} and  \cite{JohariGameTheory}, the authors study the interaction between traffic engineering and content distribution using a game-theoretic model and show that, without a joint optimization, the equilibrium of this interaction may not result in a socially optimal solution. In  \cite{Jiang2009}, it is shown that a joint optimization can achieve benefits of up to 20\% for ISPs and up to 30\% for CDNs as compared to the case when there is no cooperation between them. 

Compared to all of these works that equate content distribution to server selection (or request redirection in our parlance), the \ncp\ optimization formulation additionally considers content placement itself as a degree of freedom. As our results show, optimizing placement is powerful and can single-handedly reduce the MLU significantly even in conjunction with simplistic request redirection and routing strategies. 

%\noindent\textbf{Content Placement in ISP networks:}
% content pl
%\noindent\textbf{Network CDNs:}
%Major ISP/Telco organizations such as AT \& T \cite{att-cdn}, Level-3 \cite{level3-cdn}and Verizon \cite{verizon-cdn} are building hosting and content delivery infrastructure into their networks.  ISPs enjoy a natural advantage for a CDN since they can place content within their networks at close proximity to their own customers. ISPs can integrate CDN infrastructure such as storage and server clusters at their PoP locations. The falling prices of bandwidth are providing the push to these companies to enter CDN market. 

\paragraph{Placement Optimization}
Applegate et al. \cite{Applegate2010} study the content placement problem for a VoD system that seeks to minimize the aggregate network bandwidth consumed assuming a {\em fixed routing} in the network.  They  compare different video placement algorithms and find that an optimized, demand-aware placement strategy with a small local cache (similar in spirit to our "hybrid'' strategy in $\S$\ref{sec:hybrid}) outperforms purely demand-oblivious LRU-like strategies. Our work differs from theirs with respect to both the problem addressed and the qualitative findings as follows. We model an \ncp\ that in addition to placement also controls routing, and assessing the interaction and relative importance of routing and placement strategies is one of our contributions. 

Furthermore, unlike \cite{Applegate2010}, we find that a simple, demand-oblivious, LRU-like strategy significantly outperforms an optimized, demand-aware (static or hybrid) placement strategy. There are two explanations for this seeming disparity. First, we consider a comprehensive trace of CDN requests with a variety of on-demand video as well as download traffic that exhibits significant daily churn; in comparison, their workload appears to be reasonably predictable even over weekly timescales. Second, the optimized, demand-aware placement strategy they consider also has some benefit of future knowledge and is therefore somewhat comparable to the optimized scheme we analyze with future knowledge (OPT-R/OPT-P/Future in $\S$\ref{sec:eval}). In general, obtaining knowledge about future demand may not be practical for all types of content, e.g., news videos, for a large \ncp.  Furthermore, our analysis of different storage ratios suggests that LRU performs worse only at small storage ratios,  and the difference between  LRU and optimized content placement reduces significantly on increasing the storage ratio.

% of additional storage makes it comparable to an optimized scheme with near-perfect future knowledge. 

%In this paper, we study the relative importance of optimizing content placement, flow split and routing on the backbone traffic in an ISP network, and not just a comparison of different content placement algorithms.


\paragraph{Traffic Engineering}  Traffic engineering schemes have seen a long line of work and a variety of schemes such as OSPF link-weight optimization \cite{fortz2000internet}, MPLS flow splitting \cite{MPLS2},  optimizing routing using multiple traffic matrices \cite{COPE, MultiTM}, online engineering strategies \cite{TEXCP,MPLS2}, and provably near-optimal oblivious routing \cite{Cohen,Racke} have been studied. All of these schemes assume that the demand traffic is a given to which routing must adapt. However, we find that \ancp\ is in a powerful position to change the demand traffic matrix, so much so that even a naive scheme like InvCap, i.e., no engineering at all, suffices in conjunction with a judicious placement strategy and optimizing routing further adds little value. In this respect, our findings are comparable in spirit to Abhigyan et al. \cite{Abhigyan}. However, they focus on the impact of location diversity, assuming a fixed number of randomly placed replicas of each content, and find that even a small number of random replicas suffice to  blur differences between different traffic engineering schemes with respect to a capacity metric (incomparable to MLU), but find that engineering schemes still outperform static InvCap routing. 


\eat
{
\paragraph{Content-aware Traffic Engineering} 

In CaTE \cite{CATE}, authors evaluate the benefits of content aware traffic engineering for a variety of objectives, such as network cost, latency etc. CaTE's model assumes a co-operation between an ISP and top content providers (CPs) in terms of the volume of traffic on the ISP network. CPs servers are available at multiple locations in the Internet, an observation which the authors substantiate based on traces collected at multiple PoPs of a Tier1 ISP.  They find that if top 10 major CPs make optimal request redirection decisions based on network information provided by ISPs, it can reduce network cost for ISPs as well as latency for the CPs.  In contrast, we study the effect of routing and content placement, in addition to request redirection for a NCDN. Our findings show that demand oblivious content placement and routing, along with simple redirection strateiges suffice to optimize network cost for a CDN. 

}
%OSPF, MPLS based schemes, online schemes. these schemes are based on a notion of traffic matrix which is known apriori. in our model traffic matrix is not fixed but it depends on the content placement and flow split in the network. Our work compares the performance of naive traffic engineering InvCap and Optimal traffic engineering in a network for a variety of content placement and flow split strategies. we arrive at the conclusion that traditonal traffic engineering makes little different when content placement and flow split is optimized in the network.

%\noindent\textbf{Facility Location:} \tbd{Ramesh: do you want to write something here?}



\section{Swarming Systems}
\label{sec:swarming}

%BitTorrent, the brainchild of Bram Cohen, has achieved tremendous success and spanned a vast literature in the previous decade~\cite{Qiu-Srikant,  bittorrent, laoutaris, bittorrent}.  
%In 2005, Bram Cohen pointed out that ``making seeding optimizations for enterprise use'' was one of the ``puzzles'' to be solved to improve the BitTorrent protocol~\cite{coheninter}.  Interestingly, despite the growing interest on the use of peer-to-peer swarming in the enterprise domain (due to cost reductions in bandwidth~\cite{wwcraft, wikipediap2p, linux} and energy~\cite{laoutaris}) and the popularity of server-assisted peer-to-peer systems (such as Shark~\cite{shark}, Coral~\cite{coral} and Coblitz~\cite{coblitz}) the literature on the enterprise use of peer-to-peer systems is still scarce and is comprised mainly of white papers~\cite{computing, kontiki, akamai} (some notable exceptions being~\cite{antfarm, chow, ioannidis, performance2010}).   

Managed or server-assisted peer-to-peer swarming has received considerable attention from researchers \cite{antfarm, chow,daskleinrock, onserverdimensioning, performance2010, shakkottai} as well as the industry \cite{octoshape,amazons3,kontiki, Akamai} in the last few years. The research literature on bandwidth allocation strategies for managed swarms can be broadly classified as being based on online measurement \cite{antfarm,vformation,chow} or based on analytical modeling \cite{daskleinrock, onserverdimensioning, performance2010, shakkottai}. In comparison, our work focuses on comparing allocation strategies based on online measurement and models based on offline measurement.

Our work is closely related to recent work by Peterson et al. on AntFarm \cite{antfarm} and follow-on work \cite{vformation} that improves upon AntFarm. However, our goals and approach differ significantly from those of AntFarm. As described in Section \ref{sec:bg}, our goal is to develop a solution that is protocol-compatible, interoperates with legacy BitTorrent clients, and requires minimal information from peers. On the other hand, a good bit of AntFarm's design stems from its goal of thwarting selfish peer behavior. Consequently, AntFarm uses a custom wireline protocol and fine-grained information about peer uploads and downloads in conjunction with a virtual currency mechanism to limit selfish behavior. Our position is that a large majority of users use BitTorrent clients as-is or use unmodifiable closed-source clients, so incentive issues are less important in such scenarios and managed swarming scenarios in general.

The optimization problem posed in our work at a high-level seems similar to that addressed by AntFarm, however there are some crucial differences. First, AntFarm seeks to optimize the {\em aggregate bandwidth} across peers, whereas \cs\ seeks to optimize a variety of {\em per-peer} performance metrics. While this difference may seem like a technicality, it significantly changes the underlying problem. The aggregate bandwidth in a swarm that is stable, i.e., the arrival rate and departure rate are roughly matched in steady state, is simply given by $\lambda S$ where $\lambda$ is the arrival rate. So the aggregate steady-state bandwidth can not be ``controlled'' if the arrival rate of peers remains unchanged. 

The experiments in the AntFarm paper suggest that they primarily focus on flash-crowd scenarios where a fixed number, $N$, of peers arrive at the beginning of the swarm and download a sufficiently long file during which $N$ remains fixed, i.e., there are no departures in the regime of interest. In such scenarios, the aggregate bandwidth is simply given by $N x$ where $x$ is the server bandwidth, i.e., the underlying response curve is essentially linear. Their experiments (e.g., refer Figure 8 in \cite{antfarm}) confirm the linearity of the response curves.  In contrast, \cs\ optimizes for scenarios where there are both arrivals and departures and the response curves are indeed concave (as alluded to in the conceptual part of the AntFarm paper). The number of peers $N$ in our targeted scenarios is not fixed and is in fact influenced by the server bandwidth $x$, which makes the optimization problem much more challenging.  Most importantly, a significant focus of our work is on developing and  comparing offline or model-based approaches to optimize multi-swarm performance metrics in addition to approaches based on dynamic control based on online measurement.



%The  literature on allocation strategies for server-assisted peer-to-peer content delivery encompasses measurement oriented works \cite{antfarm, chow} as well as analytical modeling efforts~\cite{daskleinrock, onserverdimensioning, performance2010, shakkottai}.  To the best of our knowledge, this paper is the first to bridge the gap between these two approaches.  

%The methodology adopted in measurement-oriented works \cite{antfarm, chow} consists of probing the swarms to infer the gains of modifying the bandwidth allocation.  The model-driven efforts~\cite{daskleinrock, onserverdimensioning, performance2010, shakkottai}, in contrast, usually consist of defining an optimization problem to be solved by the publishers and then showing how different system parameters affect the optimal bandwidth allocation strategy.  The approach we take in this paper is a mix of these two.  We couple off-line measurements and insights on how different system parameters affect bandwidth allocation with on-line probing in order to split the server bandwidth across multiple swarms.  

Ioannidis et al.~\cite{ioannidis} study how quickly the bandwidth available at the server has to grow as the number of users increases.  Hajek and Zhou~\cite{hajek} address  a question that, in nature, resembles the one in~\cite{ioannidis}, and show conditions under which the server-assisted peer-to-peer system is stable, i.e., the population size remains bounded as a function of time.  In this work, we assume that the system under consideration is always under a stable regime.   In particular, we empirically observe that stability issues do not affect our results for the parameters and time spans considered in this paper, and believe that the concern arises only in scenarios when the server bandwidth is unrealistically small.

%Many recent works have studied how collaborations across swarms together with server capacity allocation can improve the overall system performance~\cite{levtov,carlsson, yang}.  The key idea consists of incentivizing peers to exchange content that they did not request, which  yields  decreased mean download times due to availability gains.   These mechanisms inevitably require modifications to the BitTorrent clients as well as strategies to enable transactions across swarms.   In this paper, we showed that simple server bandwidth allocation strategies in many case suffice in order to achieve the desired performance goals, requiring no change to existing BitTorrent clients.  This makes our proposed strategies suitable to deployment in systems that already natively  support BitTorrent such as the Amazon S3 Cloud~\cite{amazons3}.  

There are two kinds of seeding strategies relevant in swarming systems:  inter-swarm and  intra-swarm allocation of capacity to peers.  In this paper we focused on the former, relying on the intra-swarm strategies pre-built in the mainline BitTorrent.  The key element of the intra-swarm seeding strategy implemented in the mainline BitTorrent is referred to as \emph{super seeding}~\cite{wikipedia}, which attempts to minimize the amount of data uploaded by a seed.  When peers arrive to an under-populated swarm, the seed claims to have no pieces of the content.  As peers connect to it, it informs that it received a new piece and allows a peer to download it.  The seed does not upload any other piece to a peer until other peers advertise that they received that piece.  Super seeding, as well as other works that proposed alternative intra-swarm seeding strategies~\cite{bharambe05, legout:07, chow}, are complementary to ours.   While~\cite{wikipedia, bharambe05, legout:07, chow} focuses on incentive mechanisms to enforce that peers contribute to the system, in this paper our concern is with performance under the assumption that clients do not misbehave.

%Online storage mechanisms can also benefit from the mechanisms proposed in this paper.   Systems such as FS2You~\cite{fs2you} and BitStore~\cite{bitstore} leverage peer-assisted online storage, focusing on content availability but not discussing how seeds should allocate bandwidth across multiple files.   We believe that some of the ideas presented in this paper might be adapted and extended to ~\cite{fs2you, bitstore}, accounting  for the fact that in peer-assisted online storage systems peers might have incentives to stay online after completing their downloads.  

%In this paper we proposed mechanisms for server capacity allocation for the transmission of stored content.  The order at which packets are delivered, as well as strict delivery deadlines, are not relevant.  Future work consists of extending the ideas presented in this paper to peer-to-peer live streaming systems~\cite{contracts,massoulietwigg, tawarikleinrock}.   Extending our work to trackerless systems, which rely on random contacts between peers for content dissemination~\cite{chaintreau1, chaintreau2}, is also subject of future work.




%\section{Limitations and Future Work}
%
%In this section, we discuss limitations of our \ncp\ model and our experimental approach. 
%
%(1) Our \ncp\ model models only the CDN traffic and not the transit traffic which present in an ISP network. Since, an ISP cannot control content placement for transit traffic, traffic engineering will help reduce network cost for transit traffic.
%
%(2) Minimizing network cost in face of link failures, either planned or accidental, is one of the goals of traffic engineering. \Ancp\ could suffer from link failures as well as server failures at the PoPs. We intend to study the effect of link failures as well as server failures for \ancp\ in our future work.

\section{Application centric comparison}
\label{sec:rel}

%\section{Background}


\textbf{Traffic engineering:}
The past decade has seen considerable work in the area of traffic engineering seeking to optimize link utilization based metrics using OSPF \cite{fortz2000internet} or MPLS \cite{MPLSIntro}.  TE techniques can be broadly classified as {\em offline}, {\em online}, or {\em oblivious}. Offline TE is done using measured TMs by ISPs and is widely deployed today \cite{rexford}. Different approaches to offline TE include optimizing OSPF link weights \cite{fortz2000internet}; optimizing over multiple TMs \cite{MultiTM};  optimizing  for unpredictable traffic demands \cite{COPE}, and so on. In contrast, online TE  computes routes using online measurements of network conditions \cite{MPLS2,COPE}. The main argument in favor of online TE is that their responsiveness at short time scales can enable them to achieve costs close to the optimal. In contrast to offline or online TE, oblivious routing algorithms seek to compute routes that perform well across all possible traffic matrices thereby obviating TE \cite{Cohen}. 

In contrast to all of this prior work, our work studies TE focusing on application performance metrics instead of link utilization based metrics.
Prior work has recognized that TE schemes can increase path delay, e.g.. \cite{TEXCP}.  To the best of our knowledge, our work is the first to empirically quantify the impact on TCP throughput and to show that engineering for rare traffic spikes comes at the cost of hurting common-case TCP throughput. He et al \cite{JointCCR2} consider the joint optimization of congestion control and routing and propose a distributed online solution that optimizes an aggregate utility function of TE cost and user-perceived performance. In comparison, we empirically study the interaction of TE and application adaptation with network and transport layer protocols that are widely deployed today.



\textbf{Interaction of location diversity and TE:}
Recent work has explored the joint optimization of TE and content distribution, i.e., choosing the best location(s) to download content. 
P4P \cite{P4P}  seeks to improve application performance for P2P traffic and also reduces cost for ISP by reducing interdomain traffic and MLU. In \cite{Jiang2009} and  \cite{JohariGameTheory}, the authors study the interaction between TE and content distribution using a game theoretic model and show that without a joint optimization, the equilibrium of this interaction may not be a socially optimal solution. The three node network in  Section~\ref{sec:background} also illustrates this point.In  \cite{Jiang2009}, it is shown that a joint optimization can achieve benefits of up to 20\% for ISPs and up to 30\% for CDNs as compared to the case when there is no cooperation between them. 
While such coordination-based proposals may be adopted in future, TE and content distribution are done by separate entities today. Our work studies this interaction in the present setting and shows that even without a joint optimization approach, location diversity increases  capacity for TE schemes by up to 2$\times$ and enables all TE schemes to achieve near-optimal capacity.


\textbf{Path diversity:}
Path diversity is another form of application adaptation which has been explored in research, e.g., detour routing  \cite{Detour}, and is used in Internet today, e.g. Akamai   \cite{Akamai} which uses detour routes for data transfer within its network of servers, and Skype \cite{Skype}, the popular VoIP application, which uses an overlay network to route data. Our focus in this paper is on location diversity. We intend to analyze the effect of adaptation in the form of path diversity as a part of our future work.
